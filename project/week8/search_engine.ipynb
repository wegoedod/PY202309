{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "453dace9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 쿼리를 입력하세요.THe SEa\n",
      "rank\tIndex\tscore\tsentence\n",
      "1\t115\t0.2\tThe parks are beautiful.\n",
      "2\t121\t0.2\tThe boss replied, \"Really?\"\n",
      "3\t178\t0.2\tThis puzzled the pig.\n",
      "4\t656\t0.2\tThe boy sat down.\n",
      "5\t11\t0.16666666666666666\tIt shines over the sea.\n",
      "6\t56\t0.16666666666666666\tNext comes the \"horror\" stage.\n",
      "7\t117\t0.16666666666666666\tNow, it's the lion's turn.\n",
      "8\t228\t0.16666666666666666\tThe boy said, \"Come in.\"\n",
      "9\t351\t0.16666666666666666\t\"Do you have the time?\"\n",
      "10\t479\t0.16666666666666666\tThe second tried counting sheep.\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# 문장을 전처리하여 공백을 기준으로 토큰화하는 함수\n",
    "def preprocess(sentence):\n",
    "    preprocessed_sentence = sentence.strip().split(\" \")\n",
    "    return preprocessed_sentence\n",
    "\n",
    "# 파일을 읽어서 토큰화된 문장 쌍을 생성하는 함수\n",
    "def indexing(file_name):\n",
    "    file_tokens_pairs = []\n",
    "    file_tokens_pairs_lower = []\n",
    "    lines = open(file_name, \"r\", encoding=\"utf8\").readlines()\n",
    "    for line in lines:\n",
    "        tokens = preprocess(line) \n",
    "        tokens_lower = preprocess(line.lower()) #preprocess함수 호출 #lower함수로 대소문자 구분 안 함\n",
    "        file_tokens_pairs.append(tokens)\n",
    "        file_tokens_pairs_lower.append(tokens_lower)\n",
    "    return file_tokens_pairs, file_tokens_pairs_lower\n",
    "\n",
    "# 쿼리와 문장 간의 유사도를 계산하는 함수\n",
    "def calc_similarity(preprocessed_query, preprocessed_sentence):\n",
    "    score_dict = {}\n",
    "    for i in range(len(preprocessed_sentence)): \n",
    "        file_token_set = set(preprocessed_sentence[i])\n",
    "        all_tokens = preprocessed_query | file_token_set#file 단어 수 + query 단어 수 - 겹치는 수\n",
    "        same_tokens = preprocessed_query & file_token_set#겹치는 수\n",
    "        similarity = len(same_tokens) / len(all_tokens)\n",
    "        score_dict[i] = similarity #score_dict[i]에 저장\n",
    "    return score_dict\n",
    "\n",
    "# 1. Indexing\n",
    "## https://github.com/jungyeul/korean-parallel-corpora\n",
    "file_name = \"jhe-koen-dev.en\"\n",
    "file_tokens_pairs,  file_tokens_pairs_lower= indexing(file_name)\n",
    "\n",
    "\n",
    "# 2. 사용자로부터 쿼리 입력\n",
    "query = input(\"영어 쿼리를 입력하세요.\")\n",
    "preprocessed_query = preprocess(query)\n",
    "preprocessed_query_lower = preprocess(query.lower())#lower함수로 대소문자 구분 안 함 \n",
    "query_token_set = set(preprocessed_query)\n",
    "query_token_set_lower = set(preprocessed_query_lower)\n",
    "\n",
    "# 3. 동일 토큰 집합을 기반으로 유사도 계산\n",
    "score_dict = calc_similarity(query_token_set_lower, file_tokens_pairs_lower)\n",
    "\n",
    "# 4. 유사도 리스트 정렬\n",
    "sorted_score_list = sorted(score_dict.items(), key = operator.itemgetter(1), reverse=True)\n",
    "\n",
    "# 5. 결과 출력\n",
    "if sorted_score_list[0][1] == 0.0:\n",
    "    print(\"There is no similar sentence.\")\n",
    "else:\n",
    "    print(\"rank\", \"Index\", \"score\", \"sentence\", sep = \"\\t\")\n",
    "    rank = 1\n",
    "    for i, score  in sorted_score_list:\n",
    "        print(rank, i, score, ' '.join(file_tokens_pairs[i]), sep = \"\\t\")\n",
    "        if rank == 10:\n",
    "            break\n",
    "        rank = rank + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
